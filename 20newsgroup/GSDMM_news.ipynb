{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_4hHWSCeAbF"
      },
      "source": [
        "Source: https://towardsdatascience.com/end-to-end-topic-modeling-in-python-latent-dirichlet-allocation-lda-35ce4ed6b3e0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pip install git+https://github.com/rwalk/gsdmm.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "M8R4IfSWpo5d"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import io\n",
        "from gsdmm import MovieGroupProcess\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "import gensim\n",
        "from gensim import corpora\n",
        "from collections import defaultdict\n",
        "from gensim.models.coherencemodel import CoherenceModel\n",
        "from gensim.corpora.dictionary import Dictionary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 1: Load the dataset\n",
        "newsgroups = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))\n",
        "\n",
        "# Step 2: Create a DataFrame for better handling\n",
        "df = pd.DataFrame({\n",
        "    'Text': newsgroups.data,\n",
        "    'Category': [newsgroups.target_names[label] for label in newsgroups.target]\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(18846, 2)\n",
            "Median Word Count: 83.0\n",
            "Minimum Word Count: 19\n",
            "Maximum Word Count: 330\n",
            "(15021, 3)\n"
          ]
        }
      ],
      "source": [
        "print(df.shape)\n",
        "\n",
        "# Step 1: Count words in the 'Text' column\n",
        "df['WordCount'] = df['Text'].apply(lambda x: len(x.split()))\n",
        "\n",
        "# Step 2: Calculate the thresholds for the bottom and top 10%\n",
        "lower_threshold = df['WordCount'].quantile(0.1)\n",
        "upper_threshold = df['WordCount'].quantile(0.9)\n",
        "\n",
        "# Step 3: Filter the DataFrame to exclude bottom 10% and top 10%\n",
        "df = df[(df['WordCount'] > lower_threshold) & (df['WordCount'] < upper_threshold)]\n",
        "\n",
        "# Step 2: Calculate statistics\n",
        "median_word_count = df['WordCount'].median()\n",
        "min_word_count = df['WordCount'].min()\n",
        "max_word_count = df['WordCount'].max()\n",
        "\n",
        "# Display the results\n",
        "print(\"Median Word Count:\", median_word_count)\n",
        "print(\"Minimum Word Count:\", min_word_count)\n",
        "print(\"Maximum Word Count:\", max_word_count)\n",
        "\n",
        "# Display the filtered DataFrame\n",
        "print(df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(493, 2)"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = df = pd.read_csv('../Data/20NewsGroup_500.csv')\n",
        "df.dropna(subset=['Text'], inplace=True)\n",
        "# Randomly sample 500 rows (use the actual dataset size if it's smaller than 500)\n",
        "#df = df.sample(n=min(500, len(df)), random_state=42)\n",
        "\n",
        "# Rename 'title' column to 'Comment'\n",
        "df.rename(columns={'Text': 'Comment'}, inplace=True)\n",
        "\n",
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qpz4pIbe3rQN",
        "outputId": "318e8a57-0923-4825-eb63-1f4d274106ec"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /Users/sajjadislam/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     /Users/sajjadislam/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# Download stopwords\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Initialize stop words and lemmatizer\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# def preprocess(text):\n",
        "#     stop_words = set(stopwords.words('english'))\n",
        "#     words = word_tokenize(text.lower())\n",
        "#     return [word for word in words if word.isalpha() and word not in stop_words]\n",
        "\n",
        "def preprocess(text):\n",
        "    # Tokenize, remove stopwords and lemmatize\n",
        "    return [lemmatizer.lemmatize(word) for word in gensim.utils.simple_preprocess(text) if word not in stop_words]\n",
        "\n",
        "processed_data = [preprocess(text) for text in df['Comment']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fGmR0BE83rQN",
        "outputId": "ab6e588c-bfcf-4e9b-86c3-83d93bf40a3d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "In stage 0: transferred 449 clusters with 20 clusters populated\n",
            "In stage 1: transferred 221 clusters with 19 clusters populated\n",
            "In stage 2: transferred 96 clusters with 14 clusters populated\n",
            "In stage 3: transferred 76 clusters with 12 clusters populated\n",
            "In stage 4: transferred 63 clusters with 12 clusters populated\n",
            "In stage 5: transferred 53 clusters with 10 clusters populated\n",
            "In stage 6: transferred 57 clusters with 10 clusters populated\n",
            "In stage 7: transferred 48 clusters with 9 clusters populated\n",
            "In stage 8: transferred 41 clusters with 11 clusters populated\n",
            "In stage 9: transferred 41 clusters with 7 clusters populated\n",
            "In stage 10: transferred 39 clusters with 9 clusters populated\n",
            "In stage 11: transferred 46 clusters with 8 clusters populated\n",
            "In stage 12: transferred 44 clusters with 9 clusters populated\n",
            "In stage 13: transferred 34 clusters with 10 clusters populated\n",
            "In stage 14: transferred 40 clusters with 7 clusters populated\n",
            "Coherence Score: 0.5941684511114177\n"
          ]
        }
      ],
      "source": [
        "# Create an instance of the GSDMM model\n",
        "# num_topics = k = 5\n",
        "mgp = MovieGroupProcess(K=20, alpha=0.1, beta=0.1, n_iters=15)\n",
        "\n",
        "# Fit the model on the data\n",
        "vocab = set(x for doc in processed_data for x in doc)\n",
        "n_terms = len(vocab)\n",
        "y = mgp.fit(processed_data, n_terms)\n",
        "\n",
        "# To see the topics for the first 10 documents\n",
        "#for i in range(5):\n",
        "    #print(f\"Document: {processed_data[i]}, Topic: {mgp.choose_best_label(processed_data[i])}\")\n",
        "\n",
        "\n",
        "# Assuming mgp is your fitted GSDMM model and processed_data is your documents\n",
        "\n",
        "# Find the dominant topic for each document\n",
        "doc_topic = [mgp.choose_best_label(doc) for doc in processed_data]\n",
        "\n",
        "# Word frequencies per topic\n",
        "topic_word_freq = defaultdict(lambda: defaultdict(int))\n",
        "for doc, topic in zip(processed_data, doc_topic):\n",
        "    for word in doc:\n",
        "        topic_word_freq[topic[0]][word] += 1\n",
        "\n",
        "# Extract top N words for each topic\n",
        "top_n = 5\n",
        "top_words_per_topic = {}\n",
        "for topic, word_freq in topic_word_freq.items():\n",
        "    top_words = sorted(word_freq, key=word_freq.get, reverse=True)[:top_n]\n",
        "    top_words_per_topic[topic] = top_words\n",
        "\n",
        "# Create a dictionary and corpus for coherence calculation\n",
        "dictionary = Dictionary(processed_data)\n",
        "corpus = [dictionary.doc2bow(doc) for doc in processed_data]\n",
        "\n",
        "#list(top_words_per_topic.values())\n",
        "\n",
        "# Using the top words for each topic to calculate coherence\n",
        "coherence_model = CoherenceModel(topics=list(top_words_per_topic.values()),texts=processed_data,dictionary=dictionary, coherence='c_v')\n",
        "#coherence_model = CoherenceModel(topics=list(top_words_per_topic.values()), corpus = corpus, dictionary=dictionary, coherence='u_mass')\n",
        "#coherence_model = CoherenceModel(topics=list(top_words_per_topic.values()), texts=processed_data, dictionary=dictionary, coherence='c_uci')\n",
        "coherence_score = coherence_model.get_coherence()\n",
        "print('Coherence Score:', coherence_score)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "py312_topic_modeling",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
