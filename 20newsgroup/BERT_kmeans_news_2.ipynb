{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124\n",
    "\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nipua\\AppData\\Local\\anaconda3\\envs\\py312_gpu\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.models import CoherenceModel\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "from collections import Counter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Ensure GPU availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load the dataset\n",
    "newsgroups = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "# Step 2: Create a DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'Text': newsgroups.data,\n",
    "    'Category': [newsgroups.target_names[label] for label in newsgroups.target]\n",
    "})\n",
    "\n",
    "df.dropna(subset=['Text'], inplace=True)\n",
    "\n",
    "# Step 3: Count words and filter by word count\n",
    "df['WordCount'] = df['Text'].apply(lambda x: len(x.split()))\n",
    "lower_threshold = df['WordCount'].quantile(0.1)\n",
    "upper_threshold = df['WordCount'].quantile(0.9)\n",
    "df = df[(df['WordCount'] > lower_threshold) & (df['WordCount'] < upper_threshold)]\n",
    "\n",
    "# Randomly sample 500 rows (use the actual dataset size if it's smaller than 500)\n",
    "#df = df.sample(n=min(10000, len(df)), random_state=42)\n",
    "\n",
    "df.rename(columns={'Text': 'Comment'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased').to(device)\n",
    "\n",
    "# Function to encode comments into embeddings\n",
    "def encode_comments_in_batches(texts, batch_size=16):\n",
    "    all_embeddings = []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch_texts = texts[i:i+batch_size]\n",
    "        encoded_input = tokenizer(batch_texts, padding=True, truncation=True, return_tensors='pt').to(device)\n",
    "        with torch.no_grad():\n",
    "            model_output = model(**encoded_input)\n",
    "        embeddings = model_output.last_hidden_state.mean(dim=1).cpu()\n",
    "        all_embeddings.append(embeddings)\n",
    "    # Concatenate all embeddings\n",
    "    return torch.cat(all_embeddings, dim=0)\n",
    "\n",
    "# Optional: Minimal text cleaning (if necessary)\n",
    "def clean_text(text):\n",
    "    return text.replace('\\n', ' ').strip()\n",
    "\n",
    "# Apply minimal cleaning\n",
    "cleaned_texts = [clean_text(text) for text in df['Comment']]\n",
    "# Encode comments\n",
    "embeddings = encode_comments_in_batches(cleaned_texts, batch_size=16)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coherence Score: 0.3516313814445421\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Perform clustering on embeddings\n",
    "num_topics = 40  # Set the number of topics\n",
    "kmeans = KMeans(n_clusters=num_topics, random_state=0).fit(embeddings.numpy())\n",
    "df['topic'] = kmeans.labels_\n",
    "\n",
    "# Step 5: Calculate coherence score\n",
    "# Transform text to bag-of-words format for coherence calculation\n",
    "texts = [comment.split() for comment in df['Comment'].tolist()]\n",
    "dictionary = Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "# Function to get representative comments for a topic\n",
    "def get_representative_comments(topic_idx, n_representative=150):\n",
    "    indices = np.where(df['topic'] == topic_idx)[0]\n",
    "    centroid = kmeans.cluster_centers_[topic_idx]\n",
    "    distances = cosine_similarity([centroid], embeddings[indices].numpy())\n",
    "    closest_indices = np.argsort(distances[0])[:n_representative]\n",
    "    return [df.iloc[indices[i]]['Comment'] for i in closest_indices]\n",
    "\n",
    "# Function to find most frequent words in comments\n",
    "def get_most_frequent_words(comments, top_n=15):\n",
    "    words = [word for comment in comments for word in comment.split()\n",
    "             if word not in string.punctuation]\n",
    "    most_common = [word for word, freq in Counter(words).most_common(top_n)]\n",
    "    return most_common\n",
    "\n",
    "# Create lists of most frequent words representing each topic\n",
    "topic_words = [get_most_frequent_words(get_representative_comments(i)) for i in range(num_topics)]\n",
    "\n",
    "# Calculate coherence score\n",
    "cm = CoherenceModel(topics=topic_words, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "coherence_score = cm.get_coherence()\n",
    "print(f'Coherence Score: {coherence_score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "top 5\n",
    "\n",
    "topic 3 = 0.5119179329036139\n",
    "\n",
    "topic 5 = 0.4898835056685732\n",
    "\n",
    "topic 10 = 0.5281536171510373\n",
    "\n",
    "topic 15 = 0.5174468094419998\n",
    "\n",
    "topic 20 = 0.5182541863526895\n",
    "\n",
    "topic 25 = 0.5206877165695638\n",
    "\n",
    "topic 30 = 0.5206104688853044\n",
    "\n",
    "topic 35 = 0.5220296994075871\n",
    "\n",
    "topic 40 = 0.5216745726384806\n",
    "\n",
    "topic 45 = 0.5201528389645848\n",
    "\n",
    "\n",
    "top 10\n",
    "\n",
    "topic 3 = 0.34896567061446193\n",
    "\n",
    "topic 5 = 0.3798411251147066\n",
    "\n",
    "topic 10 = 0.3882506355244847\n",
    "\n",
    "topic 15 = 0.4012699827936591\n",
    "\n",
    "topic 20 = 0.4006519914742827\n",
    "\n",
    "topic 25 = 0.4045064945610071\n",
    "\n",
    "topic 30 = 0.40204047823245587\n",
    "\n",
    "topic 35 = 0.405453692537197\n",
    "\n",
    "topic 40 = 0.40523946667548855\n",
    "\n",
    "topic 45 = 0.4037661455376878\n",
    "\n",
    "\n",
    "top 15\n",
    "\n",
    "\n",
    "topic 5 = 0.3252013509109818\n",
    "\n",
    "topic 10 = 0.33315294315792754\n",
    "\n",
    "topic 20 = 0.34223336469297483\n",
    "\n",
    "topic 30 = 0.3479745209290377\n",
    "\n",
    "topic 40 = 0.3516313814445421\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312_topic_modeling",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
