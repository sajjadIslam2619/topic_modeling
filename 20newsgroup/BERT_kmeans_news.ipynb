{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "source:\n",
        "\n",
        "https://www.33rdsquare.com/text-analysis-topic-modeling-with-bert/?utm_source=chatgpt.com\n",
        "\n",
        "https://link.springer.com/article/10.1007/s41870-023-01268-w?utm_source=chatgpt.com\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "s1-zZAHWIj3x"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     /Users/sajjadislam/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /Users/sajjadislam/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     /Users/sajjadislam/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     /Users/sajjadislam/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from gensim.models.coherencemodel import CoherenceModel\n",
        "from gensim.corpora.dictionary import Dictionary\n",
        "\n",
        "import nltk\n",
        "from collections import Counter\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "import string\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import torch\n",
        "from gensim.models.coherencemodel import CoherenceModel\n",
        "\n",
        "# Download required NLTK resources\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 1: Load the dataset\n",
        "newsgroups = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))\n",
        "\n",
        "# Step 2: Create a DataFrame for better handling\n",
        "df = pd.DataFrame({\n",
        "    'Text': newsgroups.data,\n",
        "    'Category': [newsgroups.target_names[label] for label in newsgroups.target]\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(18846, 2)\n",
            "Median Word Count: 83.0\n",
            "Minimum Word Count: 19\n",
            "Maximum Word Count: 330\n",
            "(15021, 3)\n"
          ]
        }
      ],
      "source": [
        "print(df.shape)\n",
        "\n",
        "# Step 1: Count words in the 'Text' column\n",
        "df['WordCount'] = df['Text'].apply(lambda x: len(x.split()))\n",
        "\n",
        "# Step 2: Calculate the thresholds for the bottom and top 10%\n",
        "lower_threshold = df['WordCount'].quantile(0.1)\n",
        "upper_threshold = df['WordCount'].quantile(0.9)\n",
        "\n",
        "# Step 3: Filter the DataFrame to exclude bottom 10% and top 10%\n",
        "df = df[(df['WordCount'] > lower_threshold) & (df['WordCount'] < upper_threshold)]\n",
        "\n",
        "# Step 2: Calculate statistics\n",
        "median_word_count = df['WordCount'].median()\n",
        "min_word_count = df['WordCount'].min()\n",
        "max_word_count = df['WordCount'].max()\n",
        "\n",
        "# Display the results\n",
        "print(\"Median Word Count:\", median_word_count)\n",
        "print(\"Minimum Word Count:\", min_word_count)\n",
        "print(\"Maximum Word Count:\", max_word_count)\n",
        "\n",
        "# Display the filtered DataFrame\n",
        "print(df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(500, 3)"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.dropna(subset=['Text'], inplace=True)\n",
        "# Randomly sample 500 rows (use the actual dataset size if it's smaller than 500)\n",
        "df = df.sample(n=min(500, len(df)), random_state=42)\n",
        "\n",
        "# Rename 'title' column to 'Comment'\n",
        "df.rename(columns={'Text': 'Comment'}, inplace=True)\n",
        "\n",
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "#import torch\n",
        "\n",
        "# Ensure GPU availability\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "meoSwXL76pzh"
      },
      "outputs": [],
      "source": [
        "# Load pre-trained BERT model and tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Function to encode comments into embeddings\n",
        "def encode_comments(texts):\n",
        "    encoded_input = tokenizer(texts, padding=True, truncation=True, return_tensors='pt')\n",
        "    with torch.no_grad():\n",
        "        model_output = model(**encoded_input)\n",
        "    embeddings = model_output.last_hidden_state.mean(dim=1)\n",
        "    return embeddings\n",
        "\n",
        "# Encode comments\n",
        "embeddings = encode_comments(df['Comment'].tolist())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8wDLu8szbENx",
        "outputId": "73de57eb-cdb7-4f87-ab6c-d8a6cdff4bf1"
      },
      "outputs": [],
      "source": [
        "# Initialize BERT tokenizer and model\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Set of English stopwords and initialize lemmatizer\n",
        "stop_words = set(stopwords.words('english'))\n",
        "# Include all digits and symbols\n",
        "custom_stop_words = set(string.digits + string.punctuation) \n",
        "additional_stop_words = ['the', 'and', 'was', 'were', 'with', 'a']\n",
        "\n",
        "stop_words.update(custom_stop_words)\n",
        "stop_words.update(additional_stop_words)\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def encode_comments(texts):\n",
        "    # Function to remove stopwords\n",
        "    def remove_stopwords(text):\n",
        "        word_tokens = word_tokenize(text)\n",
        "        filtered_text = [word for word in word_tokens if word.lower() not in stop_words]\n",
        "        return \" \".join(filtered_text)\n",
        "\n",
        "    # Function to remove stopwords and lemmatize\n",
        "    import string\n",
        "\n",
        "    def preprocess_text(text):\n",
        "        # Remove punctuation from text\n",
        "        translator = str.maketrans('', '', string.punctuation)\n",
        "        text = text.translate(translator).lower()  # Lowercasing the text here\n",
        "\n",
        "        word_tokens = word_tokenize(text)\n",
        "        filtered_and_lemmatized_text = [\n",
        "            lemmatizer.lemmatize(word)\n",
        "            for word in word_tokens\n",
        "            if word not in stop_words\n",
        "        ]\n",
        "        return \" \".join(filtered_and_lemmatized_text)\n",
        "\n",
        "    # Remove stopwords from each comment\n",
        "    cleaned_texts = [preprocess_text(text) for text in texts]\n",
        "\n",
        "    # Tokenization and encoding\n",
        "    encoded_input = tokenizer(cleaned_texts, padding=True, truncation=True, return_tensors='pt')\n",
        "    with torch.no_grad():\n",
        "        model_output = model(**encoded_input)\n",
        "    embeddings = model_output.last_hidden_state.mean(dim=1)\n",
        "    return embeddings\n",
        "\n",
        "# Encode comments\n",
        "embeddings = encode_comments(df['Comment'].tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize BERT tokenizer and model\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased').to(device)\n",
        "\n",
        "# Stop words and lemmatizer\n",
        "stop_words = set(stopwords.words('english')).union(set(string.digits + string.punctuation))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def preprocess_text(text):\n",
        "    translator = str.maketrans('', '', string.punctuation)\n",
        "    text = text.translate(translator).lower()\n",
        "    word_tokens = word_tokenize(text)\n",
        "    filtered_text = [\n",
        "        lemmatizer.lemmatize(word) for word in word_tokens if word not in stop_words\n",
        "    ]\n",
        "    return \" \".join(filtered_text)\n",
        "\n",
        "# Step 3: Preprocess comments\n",
        "df['Cleaned_Comment'] = df['Comment'].apply(preprocess_text)\n",
        "\n",
        "def encode_comments_batch(texts, batch_size=16):\n",
        "    embeddings = []\n",
        "    for i in range(0, len(texts), batch_size):\n",
        "        batch_texts = texts[i:i + batch_size]\n",
        "        encoded_input = tokenizer(batch_texts, padding=True, truncation=True, return_tensors='pt').to(device)\n",
        "        with torch.no_grad():\n",
        "            model_output = model(**encoded_input)\n",
        "        batch_embeddings = model_output.last_hidden_state.mean(dim=1).cpu()\n",
        "        embeddings.append(batch_embeddings)\n",
        "    return torch.cat(embeddings, dim=0)\n",
        "\n",
        "# Encode comments in batches\n",
        "embeddings = encode_comments_batch(df['Cleaned_Comment'].tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rBRy7kOhSt-M",
        "outputId": "58c9f7d6-c982-4286-b431-ad4dda7fe1de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Topic Words by Topic:\n",
            "Topic 1: new, model, also, sale, --, offer, monitor, '', brand, controller\n",
            "Topic 2: --, would, n't, like, 'm, ``, '', 's, know, mail\n",
            "Topic 3: n't, 's, ``, '', chronic, lambda, l, think, two, want\n",
            "Topic 4: --, '', 's, dk, ``, would, n't, use, ..., like\n",
            "Topic 5: --, '', ``, 's, new, boston, people, pittsburgh, chicago, los\n",
            "Topic 6: '', n't, 's, color, like, would, bus, key, 'd, dos\n",
            "Topic 7: file, ..., object, program, window, would, //, using, mywindow, mywinobj\n",
            "Topic 8: please, thanks, anyone, would, 's, --, times, know, could, first\n",
            "Topic 9: --, '', one, 's, ``, n't, includes, 'm, officer, could\n",
            "Topic 10: 's, --, '', ``, n't, one, would, god, jesus, think\n",
            "Coherence Score: 0.33834595136159534\n"
          ]
        }
      ],
      "source": [
        "# Perform clustering on embeddings (example: K-Means)\n",
        "num_topics = 10  # Set the number of topics\n",
        "kmeans = KMeans(n_clusters=num_topics, random_state=0).fit(embeddings.numpy())\n",
        "df['topic'] = kmeans.labels_\n",
        "\n",
        "# For Coherence Score, transform BERT embeddings to bag-of-words\n",
        "texts = [comment.split() for comment in df['Comment'].tolist()]\n",
        "dictionary = Dictionary(texts)\n",
        "corpus = [dictionary.doc2bow(text) for text in texts]\n",
        "\n",
        "# Use KMeans centroids to find closest comments for each topic\n",
        "def get_representative_comments(topic_idx, n_representative=20):\n",
        "    indices = np.where(df['topic'] == topic_idx)[0]\n",
        "    centroid = kmeans.cluster_centers_[topic_idx]\n",
        "    distances = cosine_similarity([centroid], embeddings[indices].numpy())\n",
        "    closest_indices = np.argsort(distances[0])[:n_representative]\n",
        "    return [df.iloc[indices[i]]['Comment'] for i in closest_indices]\n",
        "\n",
        "def get_most_frequent_words(comments, top_n=10):\n",
        "    # Tokenize words, convert to lower case, and filter out stop words and punctuation\n",
        "    words = [word for comment in comments\n",
        "             for word in word_tokenize(comment.lower())\n",
        "             if word not in stop_words and word not in string.punctuation]\n",
        "\n",
        "    # Find the most common words\n",
        "    most_common = [word for word, freq in Counter(words).most_common(top_n)]\n",
        "    return most_common\n",
        "\n",
        "# Create lists of most frequent words representing each topic\n",
        "topic_words = [get_most_frequent_words(get_representative_comments(i)) for i in range(num_topics)]\n",
        "# Print topic_words in a readable format\n",
        "print(\"Topic Words by Topic:\")\n",
        "for idx, words in enumerate(topic_words):\n",
        "    print(f\"Topic {idx + 1}: {', '.join(words)}\")\n",
        "\n",
        "cm = CoherenceModel(topics=topic_words, texts=texts, dictionary=dictionary, coherence='c_v')\n",
        "#cm = CoherenceModel(topics=topic_words, dictionary=dictionary, coherence='u_mass')\n",
        "coherence_score = cm.get_coherence()\n",
        "print(f'Coherence Score: {coherence_score}')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "py312_topic_modeling",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
