{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "source:\n",
        "\n",
        "https://www.33rdsquare.com/text-analysis-topic-modeling-with-bert/?utm_source=chatgpt.com\n",
        "\n",
        "https://link.springer.com/article/10.1007/s41870-023-01268-w?utm_source=chatgpt.com\n",
        "\n",
        "Datasource: https://www.kaggle.com/datasets/elvinrustam/books-dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "s1-zZAHWIj3x"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/sajjadislam/opt/anaconda3/envs/py312_topic_modeling/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     /Users/sajjadislam/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /Users/sajjadislam/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     /Users/sajjadislam/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     /Users/sajjadislam/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from gensim.models.coherencemodel import CoherenceModel\n",
        "from gensim.corpora.dictionary import Dictionary\n",
        "\n",
        "import nltk\n",
        "from collections import Counter\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "import string\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import torch\n",
        "\n",
        "# Download required NLTK resources\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Title</th>\n",
              "      <th>Authors</th>\n",
              "      <th>Description</th>\n",
              "      <th>Category</th>\n",
              "      <th>Publisher</th>\n",
              "      <th>Publish Date</th>\n",
              "      <th>Price</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>17456</th>\n",
              "      <td>Incidents of Travel in Central America, Chiapa...</td>\n",
              "      <td>By Stephens, John Lloyd</td>\n",
              "      <td>\"The ground was entirely new; there were no gu...</td>\n",
              "      <td>Travel , Special Interest , Adventure</td>\n",
              "      <td>Dover Publications</td>\n",
              "      <td>Sunday, June 1, 1969</td>\n",
              "      <td>Price Starting at $4.99</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75237</th>\n",
              "      <td>6 + 1 Traits of Writing: The Complete Guide, G...</td>\n",
              "      <td>By Culham, Ruth</td>\n",
              "      <td>Ideas, Organization, Voice, Word Choice, Sente...</td>\n",
              "      <td>Education , Teaching Methods &amp; Materials , Ge...</td>\n",
              "      <td>Scholastic Professional Books</td>\n",
              "      <td>Wednesday, January 1, 2003</td>\n",
              "      <td>Price Starting at $5.49</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38132</th>\n",
              "      <td>Neanderthal Book &amp; Skeleton (Hand in Hand with...</td>\n",
              "      <td>By Cumbaa, Stephen, Lafave, Kim (ILT), Hehner,...</td>\n",
              "      <td>Introducing a caveman for kids. Following the ...</td>\n",
              "      <td>Juvenile Nonfiction , Social Science , Archae...</td>\n",
              "      <td>Workman Publishing Company</td>\n",
              "      <td>Wednesday, January 1, 1997</td>\n",
              "      <td>Price Starting at $5.29</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24450</th>\n",
              "      <td>Listening to America: Twenty-Five Years in the...</td>\n",
              "      <td>By Wertheimer, Linda (EDT) and Npr (National P...</td>\n",
              "      <td>The publication of Listening to America coinci...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Houghton Mifflin</td>\n",
              "      <td>Monday, May 1, 1995</td>\n",
              "      <td>Price Starting at $6.36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28965</th>\n",
              "      <td>Baby Sister for Herry (Sesame Street Growing Up)</td>\n",
              "      <td>By Kingsley, Emily P.</td>\n",
              "      <td>A little blue monster named Herry finds life i...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Golden Press</td>\n",
              "      <td>Wednesday, August 1, 1984</td>\n",
              "      <td>Price Starting at $5.29</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   Title  \\\n",
              "17456  Incidents of Travel in Central America, Chiapa...   \n",
              "75237  6 + 1 Traits of Writing: The Complete Guide, G...   \n",
              "38132  Neanderthal Book & Skeleton (Hand in Hand with...   \n",
              "24450  Listening to America: Twenty-Five Years in the...   \n",
              "28965   Baby Sister for Herry (Sesame Street Growing Up)   \n",
              "\n",
              "                                                 Authors  \\\n",
              "17456                            By Stephens, John Lloyd   \n",
              "75237                                    By Culham, Ruth   \n",
              "38132  By Cumbaa, Stephen, Lafave, Kim (ILT), Hehner,...   \n",
              "24450  By Wertheimer, Linda (EDT) and Npr (National P...   \n",
              "28965                              By Kingsley, Emily P.   \n",
              "\n",
              "                                             Description  \\\n",
              "17456  \"The ground was entirely new; there were no gu...   \n",
              "75237  Ideas, Organization, Voice, Word Choice, Sente...   \n",
              "38132  Introducing a caveman for kids. Following the ...   \n",
              "24450  The publication of Listening to America coinci...   \n",
              "28965  A little blue monster named Herry finds life i...   \n",
              "\n",
              "                                                Category  \\\n",
              "17456              Travel , Special Interest , Adventure   \n",
              "75237   Education , Teaching Methods & Materials , Ge...   \n",
              "38132   Juvenile Nonfiction , Social Science , Archae...   \n",
              "24450                                                NaN   \n",
              "28965                                                NaN   \n",
              "\n",
              "                           Publisher                Publish Date  \\\n",
              "17456             Dover Publications        Sunday, June 1, 1969   \n",
              "75237  Scholastic Professional Books  Wednesday, January 1, 2003   \n",
              "38132     Workman Publishing Company  Wednesday, January 1, 1997   \n",
              "24450               Houghton Mifflin         Monday, May 1, 1995   \n",
              "28965                   Golden Press   Wednesday, August 1, 1984   \n",
              "\n",
              "                         Price  \n",
              "17456  Price Starting at $4.99  \n",
              "75237  Price Starting at $5.49  \n",
              "38132  Price Starting at $5.29  \n",
              "24450  Price Starting at $6.36  \n",
              "28965  Price Starting at $5.29  "
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = pd.read_csv('../Data/BooksDataset.csv')\n",
        "df.sample(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(65305, 7)"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.dropna(subset=['Description'], inplace=True)\n",
        "df.dropna(subset=['Category'], inplace=True)\n",
        "\n",
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YKWIq1vvIICs",
        "outputId": "aa83398b-887d-4d80-b7a3-cbb12425f3c4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "7      Collects poems written by the eleven-year-old ...\n",
              "8      The Russian author offers an affectionate chro...\n",
              "10     A humor classic, this tongue-in-cheek diet pla...\n",
              "11     Deadly germs sprayed in shopping malls, bomb-l...\n",
              "13     \"The Bible and the social and moral consequenc...\n",
              "                             ...                        \n",
              "837    In recent years, much attention has been lavis...\n",
              "838    One hot summer morning, William Hallberg, auth...\n",
              "840    Nine of America's most influential artists, sc...\n",
              "842    Love. Sex. Respect. Race. Politics. Unity. Spi...\n",
              "843    This book is a journey down the Italian penins...\n",
              "Name: Comment, Length: 500, dtype: object"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Keep only the first 500 rows in df\n",
        "df = df.iloc[:500]\n",
        "\n",
        "# Rename 'title' column to 'Comment'\n",
        "df.rename(columns={'Description': 'Comment'}, inplace=True)\n",
        "df['Comment']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "meoSwXL76pzh"
      },
      "outputs": [],
      "source": [
        "# Load pre-trained BERT model and tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Function to encode comments into embeddings\n",
        "def encode_comments(texts):\n",
        "    encoded_input = tokenizer(texts, padding=True, truncation=True, return_tensors='pt')\n",
        "    with torch.no_grad():\n",
        "        model_output = model(**encoded_input)\n",
        "    embeddings = model_output.last_hidden_state.mean(dim=1)\n",
        "    return embeddings\n",
        "\n",
        "# Encode comments\n",
        "embeddings = encode_comments(df['Comment'].tolist())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8wDLu8szbENx",
        "outputId": "73de57eb-cdb7-4f87-ab6c-d8a6cdff4bf1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /Users/sajjadislam/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     /Users/sajjadislam/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     /Users/sajjadislam/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# Initialize BERT tokenizer and model\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Set of English stopwords and initialize lemmatizer\n",
        "stop_words = set(stopwords.words('english'))\n",
        "# Include all digits and symbols\n",
        "custom_stop_words = set(string.digits + string.punctuation) \n",
        "additional_stop_words = ['the', 'and', 'was', 'were', 'with', 'a']\n",
        "\n",
        "stop_words.update(custom_stop_words)\n",
        "stop_words.update(additional_stop_words)\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def encode_comments(texts):\n",
        "    # Function to remove stopwords\n",
        "    def remove_stopwords(text):\n",
        "        word_tokens = word_tokenize(text)\n",
        "        filtered_text = [word for word in word_tokens if word.lower() not in stop_words]\n",
        "        return \" \".join(filtered_text)\n",
        "\n",
        "    # Function to remove stopwords and lemmatize\n",
        "    import string\n",
        "\n",
        "    def preprocess_text(text):\n",
        "        # Remove punctuation from text\n",
        "        translator = str.maketrans('', '', string.punctuation)\n",
        "        text = text.translate(translator).lower()  # Lowercasing the text here\n",
        "\n",
        "        word_tokens = word_tokenize(text)\n",
        "        filtered_and_lemmatized_text = [\n",
        "            lemmatizer.lemmatize(word)\n",
        "            for word in word_tokens\n",
        "            if word not in stop_words\n",
        "        ]\n",
        "        return \" \".join(filtered_and_lemmatized_text)\n",
        "\n",
        "    # Remove stopwords from each comment\n",
        "    cleaned_texts = [preprocess_text(text) for text in texts]\n",
        "\n",
        "    # Tokenization and encoding\n",
        "    encoded_input = tokenizer(cleaned_texts, padding=True, truncation=True, return_tensors='pt')\n",
        "    with torch.no_grad():\n",
        "        model_output = model(**encoded_input)\n",
        "    embeddings = model_output.last_hidden_state.mean(dim=1)\n",
        "    return embeddings\n",
        "\n",
        "# Encode comments\n",
        "embeddings = encode_comments(df['Comment'].tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rBRy7kOhSt-M",
        "outputId": "58c9f7d6-c982-4286-b431-ad4dda7fe1de"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/sajjadislam/opt/anaconda3/envs/py312_topic_modeling/lib/python3.12/site-packages/threadpoolctl.py:1214: RuntimeWarning: \n",
            "Found Intel OpenMP ('libiomp') and LLVM OpenMP ('libomp') loaded at\n",
            "the same time. Both libraries are known to be incompatible and this\n",
            "can cause random crashes or deadlocks on Linux when loaded in the\n",
            "same Python program.\n",
            "Using threadpoolctl may cause crashes or deadlocks. For more\n",
            "information and possible workarounds, please see\n",
            "    https://github.com/joblib/threadpoolctl/blob/master/multiple_openmp.md\n",
            "\n",
            "  warnings.warn(msg, RuntimeWarning)\n"
          ]
        }
      ],
      "source": [
        "# Perform clustering on embeddings (example: K-Means)\n",
        "num_topics = 5  # Set the number of topics\n",
        "kmeans = KMeans(n_clusters=num_topics, random_state=0).fit(embeddings.numpy())\n",
        "df['topic'] = kmeans.labels_\n",
        "\n",
        "# For Coherence Score, transform BERT embeddings to bag-of-words\n",
        "texts = [comment.split() for comment in df['Comment'].tolist()]\n",
        "dictionary = Dictionary(texts)\n",
        "corpus = [dictionary.doc2bow(text) for text in texts]\n",
        "\n",
        "# Use KMeans centroids to find closest comments for each topic\n",
        "def get_representative_comments(topic_idx, n_representative=10):\n",
        "    indices = np.where(df['topic'] == topic_idx)[0]\n",
        "    centroid = kmeans.cluster_centers_[topic_idx]\n",
        "    distances = cosine_similarity([centroid], embeddings[indices].numpy())\n",
        "    closest_indices = np.argsort(distances[0])[:n_representative]\n",
        "    return [df.iloc[indices[i]]['Comment'] for i in closest_indices]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J3ZlFFoS-lYU",
        "outputId": "48334e8f-b921-4b08-cd7e-493d42498feb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[[\"'s\", 'poker', 'world'],\n",
              " ['recipes', 'breads', 'de'],\n",
              " ['security', 'first', 'published'],\n",
              " [\"'s\", '--', 'company'],\n",
              " ['looks', 'describes', 'history']]"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def get_most_frequent_words(comments, top_n=5):\n",
        "    # Tokenize words, convert to lower case, and filter out stop words and punctuation\n",
        "    words = [word for comment in comments\n",
        "             for word in word_tokenize(comment.lower())\n",
        "             if word not in stop_words and word not in string.punctuation]\n",
        "\n",
        "    # Find the most common words\n",
        "    most_common = [word for word, freq in Counter(words).most_common(top_n)]\n",
        "    return most_common\n",
        "\n",
        "# Create lists of most frequent words representing each topic\n",
        "topic_words = [get_most_frequent_words(get_representative_comments(i)) for i in range(num_topics)]\n",
        "topic_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Remove the first row (index 0)\n",
        "#topic_words = topic_words[1:]\n",
        "\n",
        "# Output the modified list\n",
        "#print(topic_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M3jlDkyjAHN1",
        "outputId": "9ef149b5-b920-46b7-b0d1-da27c937acbf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Coherence Score: 0.4873080266755908\n"
          ]
        }
      ],
      "source": [
        "from gensim.models.coherencemodel import CoherenceModel\n",
        "\n",
        "cm = CoherenceModel(topics=topic_words, texts=texts, dictionary=dictionary, coherence='c_v')\n",
        "#cm = CoherenceModel(topics=topic_words, dictionary=dictionary, coherence='u_mass')\n",
        "coherence_score = cm.get_coherence()\n",
        "print(f'Coherence Score: {coherence_score}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "3 topic =  0.2098919420259906\n",
        "\n",
        "5 topic = 0.5330362309251153\n",
        "\n",
        "10 topic = 0.3184375779964808\n",
        "\n",
        "15 topic = 0.45240816473933504\n",
        "\n",
        "20 topic = 0.4231468971928173\n",
        "\n",
        "25 topic = 0.4100195812207564\n",
        "\n",
        "30 topic = \n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "py312_topic_modeling",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
