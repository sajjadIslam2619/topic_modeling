{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_4hHWSCeAbF"
      },
      "source": [
        "https://towardsdatascience.com/end-to-end-topic-modeling-in-python-latent-dirichlet-allocation-lda-35ce4ed6b3e0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "M8R4IfSWpo5d"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import io"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "id": "vyMdQQY44frg",
        "outputId": "9c260ccb-2bb3-4a90-e8b6-27b8808845d9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0      The nurses were exceptionally compassionate an...\n",
              "1      I appreciated the clear communication from the...\n",
              "2        The cleanliness of the hospital was remarkable.\n",
              "3           My pain was managed well throughout my stay.\n",
              "4                  I felt rushed during my consultation.\n",
              "                             ...                        \n",
              "295    Experiencing anger from a caregiver was both s...\n",
              "296    The awful experience I had with the staff has ...\n",
              "297    The delay in treatment had a harmful effect on...\n",
              "298    I hate to say it, but the service here has det...\n",
              "299    Witnessing cops within the hospital premises w...\n",
              "Name: Comment, Length: 300, dtype: object"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = pd.read_csv('../Data/patient_review_data.csv')\n",
        "df['Comment']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ypDck88P4nQV",
        "outputId": "d43fb1da-fd16-4933-f0e7-fe71c40f9b30"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0      15 Highly Important Questions About Adulthood,...\n",
              "1      250 Nuns Just Cycled All The Way From Kathmand...\n",
              "2      Australian comedians \"could have been shot\" du...\n",
              "3      Lycos launches screensaver to increase spammer...\n",
              "4      Fußball-Bundesliga 2008–09: Goalkeeper Butt si...\n",
              "                             ...                        \n",
              "495    '90s Cartoons With Human Eyes May Ruin Your Ch...\n",
              "496    Wait, WTF Is \"I Saw Mommy Kissing Santa Claus\"...\n",
              "497           First Women Win Seats in Kuwait Parliament\n",
              "498                   Were You A Scene Kid Or An Emo Kid\n",
              "499    Opposition leader takes early lead in Sierra L...\n",
              "Name: Comment, Length: 500, dtype: object"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = pd.read_csv('book_title_test_data.csv')\n",
        "# Keep only the first 500 rows in df\n",
        "df = df.iloc[:500]\n",
        "\n",
        "# Rename 'title' column to 'Comment'\n",
        "df.rename(columns={'title': 'Comment'}, inplace=True)\n",
        "df['Comment']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zEf2Pedoomux"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"./Data/patient_review_data.csv\")\n",
        "df = pd.read_csv(\"./Data/test_data.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rPdbGoQx3rQM",
        "outputId": "d163d59b-1424-420a-e4d9-6c81e1206367"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/rwalk/gsdmm.git\n",
            "  Cloning https://github.com/rwalk/gsdmm.git to /tmp/pip-req-build-8u25wyh7\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/rwalk/gsdmm.git /tmp/pip-req-build-8u25wyh7\n",
            "  Resolved https://github.com/rwalk/gsdmm.git to commit 4ad1b6b6976743681ee4976b4573463d359214ee\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from gsdmm==0.1) (1.25.2)\n",
            "Building wheels for collected packages: gsdmm\n",
            "  Building wheel for gsdmm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gsdmm: filename=gsdmm-0.1-py3-none-any.whl size=4585 sha256=e0ff82c286bb9a4f1cffa941a5e5195d457cf6cae338da28eb3277d28bbd40c3\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-b_mnqefj/wheels/da/d3/6e/a612d7cff0fcfb6470b8c113fc04931ecffb466ac19b9c5f3c\n",
            "Successfully built gsdmm\n",
            "Installing collected packages: gsdmm\n",
            "Successfully installed gsdmm-0.1\n"
          ]
        }
      ],
      "source": [
        "pip install git+https://github.com/rwalk/gsdmm.git\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zVSM02Tp3rQN"
      },
      "outputs": [],
      "source": [
        "from gsdmm import MovieGroupProcess\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "import gensim\n",
        "from gensim import corpora"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qpz4pIbe3rQN",
        "outputId": "318e8a57-0923-4825-eb63-1f4d274106ec"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ],
      "source": [
        "# Download stopwords\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Initialize stop words and lemmatizer\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# def preprocess(text):\n",
        "#     stop_words = set(stopwords.words('english'))\n",
        "#     words = word_tokenize(text.lower())\n",
        "#     return [word for word in words if word.isalpha() and word not in stop_words]\n",
        "\n",
        "def preprocess(text):\n",
        "    # Tokenize, remove stopwords and lemmatize\n",
        "    return [lemmatizer.lemmatize(word) for word in gensim.utils.simple_preprocess(text) if word not in stop_words]\n",
        "\n",
        "processed_data = [preprocess(text) for text in df['Comment']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fGmR0BE83rQN",
        "outputId": "ab6e588c-bfcf-4e9b-86c3-83d93bf40a3d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "In stage 0: transferred 263 clusters with 3 clusters populated\n",
            "In stage 1: transferred 137 clusters with 3 clusters populated\n",
            "In stage 2: transferred 114 clusters with 3 clusters populated\n",
            "In stage 3: transferred 99 clusters with 3 clusters populated\n",
            "In stage 4: transferred 86 clusters with 3 clusters populated\n",
            "In stage 5: transferred 74 clusters with 3 clusters populated\n",
            "In stage 6: transferred 75 clusters with 3 clusters populated\n",
            "In stage 7: transferred 72 clusters with 3 clusters populated\n",
            "In stage 8: transferred 68 clusters with 3 clusters populated\n",
            "In stage 9: transferred 70 clusters with 3 clusters populated\n",
            "In stage 10: transferred 82 clusters with 3 clusters populated\n",
            "In stage 11: transferred 76 clusters with 3 clusters populated\n",
            "In stage 12: transferred 72 clusters with 3 clusters populated\n",
            "In stage 13: transferred 64 clusters with 3 clusters populated\n",
            "In stage 14: transferred 69 clusters with 3 clusters populated\n",
            "In stage 15: transferred 66 clusters with 3 clusters populated\n",
            "In stage 16: transferred 60 clusters with 3 clusters populated\n",
            "In stage 17: transferred 61 clusters with 3 clusters populated\n",
            "In stage 18: transferred 59 clusters with 3 clusters populated\n",
            "In stage 19: transferred 89 clusters with 3 clusters populated\n",
            "In stage 20: transferred 78 clusters with 3 clusters populated\n",
            "In stage 21: transferred 64 clusters with 3 clusters populated\n",
            "In stage 22: transferred 54 clusters with 3 clusters populated\n",
            "In stage 23: transferred 63 clusters with 3 clusters populated\n",
            "In stage 24: transferred 70 clusters with 3 clusters populated\n",
            "In stage 25: transferred 67 clusters with 3 clusters populated\n",
            "In stage 26: transferred 60 clusters with 3 clusters populated\n",
            "In stage 27: transferred 62 clusters with 3 clusters populated\n",
            "In stage 28: transferred 66 clusters with 3 clusters populated\n",
            "In stage 29: transferred 68 clusters with 3 clusters populated\n",
            "Document: ['highly', 'important', 'question', 'adulthood', 'answered', 'michael', 'ian', 'black'], Topic: (1, 0.9999999925006586)\n",
            "Document: ['nun', 'cycled', 'way', 'kathmandu', 'new', 'delhi'], Topic: (1, 0.999969717933656)\n",
            "Document: ['australian', 'comedian', 'could', 'shot', 'apec', 'prank'], Topic: (2, 0.9999983527593924)\n",
            "Document: ['lycos', 'launch', 'screensaver', 'increase', 'spammer', 'bill'], Topic: (2, 0.9999996166228964)\n",
            "Document: ['fußball', 'bundesliga', 'goalkeeper', 'butt', 'sign', 'bayern', 'munich'], Topic: (1, 0.9999998234652534)\n"
          ]
        }
      ],
      "source": [
        "# Create an instance of the GSDMM model\n",
        "# num_topics = k = 5\n",
        "mgp = MovieGroupProcess(K=3, alpha=0.1, beta=0.1, n_iters=30)\n",
        "\n",
        "# Fit the model on the data\n",
        "vocab = set(x for doc in processed_data for x in doc)\n",
        "n_terms = len(vocab)\n",
        "y = mgp.fit(processed_data, n_terms)\n",
        "\n",
        "# To see the topics for the first 10 documents\n",
        "for i in range(5):\n",
        "    print(f\"Document: {processed_data[i]}, Topic: {mgp.choose_best_label(processed_data[i])}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mOj1XHbS3rQN",
        "outputId": "c675c23b-5300-41ba-9f60-fc1b1a26258e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[['know',\n",
              "  'based',\n",
              "  'new',\n",
              "  'life',\n",
              "  'actually',\n",
              "  'thing',\n",
              "  'question',\n",
              "  'would',\n",
              "  'like',\n",
              "  'show'],\n",
              " ['u',\n",
              "  'two',\n",
              "  'crash',\n",
              "  'iraq',\n",
              "  'court',\n",
              "  'return',\n",
              "  'fire',\n",
              "  'first',\n",
              "  'team',\n",
              "  'british'],\n",
              " ['new',\n",
              "  'make',\n",
              "  'kid',\n",
              "  'people',\n",
              "  'say',\n",
              "  'gift',\n",
              "  'fall',\n",
              "  'like',\n",
              "  'tweet',\n",
              "  'plan']]"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from collections import defaultdict\n",
        "from gensim.models.coherencemodel import CoherenceModel\n",
        "from gensim.corpora.dictionary import Dictionary\n",
        "\n",
        "# Assuming mgp is your fitted GSDMM model and processed_data is your documents\n",
        "\n",
        "# Find the dominant topic for each document\n",
        "doc_topic = [mgp.choose_best_label(doc) for doc in processed_data]\n",
        "\n",
        "# Word frequencies per topic\n",
        "topic_word_freq = defaultdict(lambda: defaultdict(int))\n",
        "for doc, topic in zip(processed_data, doc_topic):\n",
        "    for word in doc:\n",
        "        topic_word_freq[topic[0]][word] += 1\n",
        "\n",
        "# Extract top N words for each topic\n",
        "top_n = 10\n",
        "top_words_per_topic = {}\n",
        "for topic, word_freq in topic_word_freq.items():\n",
        "    top_words = sorted(word_freq, key=word_freq.get, reverse=True)[:top_n]\n",
        "    top_words_per_topic[topic] = top_words\n",
        "\n",
        "# Create a dictionary and corpus for coherence calculation\n",
        "dictionary = Dictionary(processed_data)\n",
        "corpus = [dictionary.doc2bow(doc) for doc in processed_data]\n",
        "\n",
        "list(top_words_per_topic.values())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FC78IV0M8NPW",
        "outputId": "946cc0eb-57e9-4400-8d9d-a1f6b238371b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Coherence Score: -13.869615911568884\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Using the top words for each topic to calculate coherence\n",
        "coherence_model = CoherenceModel(topics=list(top_words_per_topic.values()),texts=processed_data,dictionary=dictionary, coherence='c_v')\n",
        "#coherence_model = CoherenceModel(topics=list(top_words_per_topic.values()), corpus = corpus, dictionary=dictionary, coherence='u_mass')\n",
        "#coherence_model = CoherenceModel(topics=list(top_words_per_topic.values()), texts=processed_data, dictionary=dictionary, coherence='c_uci')\n",
        "coherence_score = coherence_model.get_coherence()\n",
        "print('Coherence Score:', coherence_score)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "py312_topic_modeling",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
