{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_4hHWSCeAbF"
      },
      "source": [
        "https://towardsdatascience.com/end-to-end-topic-modeling-in-python-latent-dirichlet-allocation-lda-35ce4ed6b3e0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pip install git+https://github.com/rwalk/gsdmm.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "M8R4IfSWpo5d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /Users/sajjadislam/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     /Users/sajjadislam/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import io\n",
        "from gsdmm import MovieGroupProcess\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "import gensim\n",
        "from gensim import corpora\n",
        "from collections import defaultdict\n",
        "from gensim.models.coherencemodel import CoherenceModel\n",
        "from gensim.corpora.dictionary import Dictionary\n",
        "\n",
        "# Download stopwords\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ypDck88P4nQV",
        "outputId": "d43fb1da-fd16-4933-f0e7-fe71c40f9b30"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('../Data/Final_Synthetic_Patient_Feedback_Dataset.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(450, 1)"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qpz4pIbe3rQN",
        "outputId": "318e8a57-0923-4825-eb63-1f4d274106ec"
      },
      "outputs": [],
      "source": [
        "# Initialize stop words and lemmatizer\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def preprocess(text):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = word_tokenize(text.lower())\n",
        "    return [word for word in words if word.isalpha() and word not in stop_words]\n",
        "\n",
        "# def preprocess(text):\n",
        "#     # Tokenize, remove stopwords and lemmatize\n",
        "#     return [lemmatizer.lemmatize(word) for word in gensim.utils.simple_preprocess(text) if word not in stop_words]\n",
        "\n",
        "processed_data = [preprocess(text) for text in df['Patient Feedback']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mOj1XHbS3rQN",
        "outputId": "c675c23b-5300-41ba-9f60-fc1b1a26258e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "In stage 0: transferred 262 clusters with 3 clusters populated\n",
            "In stage 1: transferred 105 clusters with 3 clusters populated\n",
            "In stage 2: transferred 31 clusters with 3 clusters populated\n",
            "In stage 3: transferred 17 clusters with 3 clusters populated\n",
            "In stage 4: transferred 8 clusters with 3 clusters populated\n",
            "In stage 5: transferred 10 clusters with 3 clusters populated\n",
            "In stage 6: transferred 14 clusters with 3 clusters populated\n",
            "In stage 7: transferred 4 clusters with 3 clusters populated\n",
            "In stage 8: transferred 11 clusters with 3 clusters populated\n",
            "In stage 9: transferred 9 clusters with 3 clusters populated\n",
            "In stage 10: transferred 11 clusters with 3 clusters populated\n",
            "In stage 11: transferred 14 clusters with 3 clusters populated\n",
            "In stage 12: transferred 12 clusters with 3 clusters populated\n",
            "In stage 13: transferred 15 clusters with 3 clusters populated\n",
            "In stage 14: transferred 15 clusters with 3 clusters populated\n",
            "In stage 0: transferred 326 clusters with 5 clusters populated\n",
            "In stage 1: transferred 94 clusters with 5 clusters populated\n",
            "In stage 2: transferred 33 clusters with 5 clusters populated\n",
            "In stage 3: transferred 19 clusters with 5 clusters populated\n",
            "In stage 4: transferred 21 clusters with 5 clusters populated\n",
            "In stage 5: transferred 27 clusters with 5 clusters populated\n",
            "In stage 6: transferred 22 clusters with 5 clusters populated\n",
            "In stage 7: transferred 21 clusters with 5 clusters populated\n",
            "In stage 8: transferred 18 clusters with 5 clusters populated\n",
            "In stage 9: transferred 14 clusters with 5 clusters populated\n",
            "In stage 10: transferred 22 clusters with 5 clusters populated\n",
            "In stage 11: transferred 25 clusters with 5 clusters populated\n",
            "In stage 12: transferred 21 clusters with 5 clusters populated\n",
            "In stage 13: transferred 13 clusters with 5 clusters populated\n",
            "In stage 14: transferred 16 clusters with 5 clusters populated\n",
            "In stage 0: transferred 376 clusters with 10 clusters populated\n",
            "In stage 1: transferred 107 clusters with 10 clusters populated\n",
            "In stage 2: transferred 48 clusters with 10 clusters populated\n",
            "In stage 3: transferred 29 clusters with 10 clusters populated\n",
            "In stage 4: transferred 19 clusters with 10 clusters populated\n",
            "In stage 5: transferred 26 clusters with 10 clusters populated\n",
            "In stage 6: transferred 32 clusters with 10 clusters populated\n",
            "In stage 7: transferred 25 clusters with 10 clusters populated\n",
            "In stage 8: transferred 20 clusters with 10 clusters populated\n",
            "In stage 9: transferred 22 clusters with 10 clusters populated\n",
            "In stage 10: transferred 28 clusters with 10 clusters populated\n",
            "In stage 11: transferred 25 clusters with 10 clusters populated\n",
            "In stage 12: transferred 25 clusters with 10 clusters populated\n",
            "In stage 13: transferred 26 clusters with 10 clusters populated\n",
            "In stage 14: transferred 22 clusters with 10 clusters populated\n",
            "In stage 0: transferred 384 clusters with 15 clusters populated\n",
            "In stage 1: transferred 116 clusters with 15 clusters populated\n",
            "In stage 2: transferred 50 clusters with 15 clusters populated\n",
            "In stage 3: transferred 35 clusters with 15 clusters populated\n",
            "In stage 4: transferred 27 clusters with 15 clusters populated\n",
            "In stage 5: transferred 19 clusters with 15 clusters populated\n",
            "In stage 6: transferred 19 clusters with 15 clusters populated\n",
            "In stage 7: transferred 23 clusters with 15 clusters populated\n",
            "In stage 8: transferred 19 clusters with 15 clusters populated\n",
            "In stage 9: transferred 14 clusters with 15 clusters populated\n",
            "In stage 10: transferred 13 clusters with 15 clusters populated\n",
            "In stage 11: transferred 12 clusters with 15 clusters populated\n",
            "In stage 12: transferred 13 clusters with 15 clusters populated\n",
            "In stage 13: transferred 15 clusters with 15 clusters populated\n",
            "In stage 14: transferred 17 clusters with 15 clusters populated\n",
            "In stage 0: transferred 390 clusters with 19 clusters populated\n",
            "In stage 1: transferred 112 clusters with 19 clusters populated\n",
            "In stage 2: transferred 47 clusters with 18 clusters populated\n",
            "In stage 3: transferred 35 clusters with 18 clusters populated\n",
            "In stage 4: transferred 36 clusters with 18 clusters populated\n",
            "In stage 5: transferred 26 clusters with 18 clusters populated\n",
            "In stage 6: transferred 24 clusters with 18 clusters populated\n",
            "In stage 7: transferred 24 clusters with 18 clusters populated\n",
            "In stage 8: transferred 20 clusters with 18 clusters populated\n",
            "In stage 9: transferred 23 clusters with 18 clusters populated\n",
            "In stage 10: transferred 17 clusters with 18 clusters populated\n",
            "In stage 11: transferred 23 clusters with 18 clusters populated\n",
            "In stage 12: transferred 21 clusters with 18 clusters populated\n",
            "In stage 13: transferred 20 clusters with 18 clusters populated\n",
            "In stage 14: transferred 25 clusters with 18 clusters populated\n",
            "    num_topics  top_n  coherence\n",
            "0            3      5   0.805581\n",
            "1            3     10   0.580945\n",
            "2            3     15   0.398460\n",
            "3            3     20   0.333099\n",
            "4            5      5   0.807994\n",
            "5            5     10   0.544567\n",
            "6            5     15   0.357743\n",
            "7            5     20   0.327948\n",
            "8           10      5   0.848167\n",
            "9           10     10   0.537886\n",
            "10          10     15   0.418089\n",
            "11          10     20   0.302280\n",
            "12          15      5   0.888646\n",
            "13          15     10   0.654489\n",
            "14          15     15   0.499532\n",
            "15          15     20   0.376754\n",
            "16          20      5   0.902087\n",
            "17          20     10   0.630237\n",
            "18          20     15   0.464550\n",
            "19          20     20   0.386240\n"
          ]
        }
      ],
      "source": [
        "# Define the range of num_topics and top_n values\n",
        "num_topics_list = [3, 5, 10, 15, 20]\n",
        "top_n_list = [5, 10, 15, 20]\n",
        "\n",
        "# Store coherence scores\n",
        "coherence_scores = []\n",
        "\n",
        "for num_topics in num_topics_list:\n",
        "    mgp = MovieGroupProcess(K=num_topics, alpha=0.1, beta=0.1, n_iters=15)\n",
        "    \n",
        "    # Fit the model on the data\n",
        "    vocab = set(x for doc in processed_data for x in doc)\n",
        "    n_terms = len(vocab)\n",
        "    mgp.fit(processed_data, n_terms)\n",
        "\n",
        "    # Find the dominant topic for each document\n",
        "    doc_topic = [mgp.choose_best_label(doc) for doc in processed_data]\n",
        "\n",
        "    # Word frequencies per topic\n",
        "    topic_word_freq = defaultdict(lambda: defaultdict(int))\n",
        "    for doc, topic in zip(processed_data, doc_topic):\n",
        "        for word in doc:\n",
        "            topic_word_freq[topic[0]][word] += 1\n",
        "\n",
        "    # Create dictionary and corpus\n",
        "    dictionary = Dictionary(processed_data)\n",
        "    corpus = [dictionary.doc2bow(doc) for doc in processed_data]\n",
        "\n",
        "    for top_n in top_n_list:\n",
        "        # Extract top N words for each topic\n",
        "        top_words_per_topic = {\n",
        "            topic: sorted(word_freq, key=word_freq.get, reverse=True)[:top_n]\n",
        "            for topic, word_freq in topic_word_freq.items()\n",
        "        }\n",
        "\n",
        "        # Calculate coherence score\n",
        "        coherence_model = CoherenceModel(\n",
        "            topics=list(top_words_per_topic.values()),\n",
        "            texts=processed_data,\n",
        "            dictionary=dictionary,\n",
        "            coherence='c_v'\n",
        "        )\n",
        "        coherence_score = coherence_model.get_coherence()\n",
        "\n",
        "        # Store results\n",
        "        coherence_scores.append({\n",
        "            \"num_topics\": num_topics,\n",
        "            \"top_n\": top_n,\n",
        "            \"coherence\": coherence_score\n",
        "        })\n",
        "\n",
        "# Convert results into a dataframe for better visualization\n",
        "df_coherence = pd.DataFrame(coherence_scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    num_topics  top_n  coherence\n",
            "0            3      5   0.805581\n",
            "1            3     10   0.580945\n",
            "2            3     15   0.398460\n",
            "3            3     20   0.333099\n",
            "4            5      5   0.807994\n",
            "5            5     10   0.544567\n",
            "6            5     15   0.357743\n",
            "7            5     20   0.327948\n",
            "8           10      5   0.848167\n",
            "9           10     10   0.537886\n",
            "10          10     15   0.418089\n",
            "11          10     20   0.302280\n",
            "12          15      5   0.888646\n",
            "13          15     10   0.654489\n",
            "14          15     15   0.499532\n",
            "15          15     20   0.376754\n",
            "16          20      5   0.902087\n",
            "17          20     10   0.630237\n",
            "18          20     15   0.464550\n",
            "19          20     20   0.386240\n"
          ]
        }
      ],
      "source": [
        "# Display the dataframe\n",
        "print(df_coherence)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "py312_topic_modeling",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
