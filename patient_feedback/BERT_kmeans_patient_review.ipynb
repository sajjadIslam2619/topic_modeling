{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "s1-zZAHWIj3x"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t78imQAk7Mqd",
        "outputId": "d7fc355d-c4c4-4227-b75c-1d1fdf9f69a9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0      The nurses were exceptionally compassionate an...\n",
              "1      I appreciated the clear communication from the...\n",
              "2        The cleanliness of the hospital was remarkable.\n",
              "3           My pain was managed well throughout my stay.\n",
              "4                  I felt rushed during my consultation.\n",
              "                             ...                        \n",
              "295    Experiencing anger from a caregiver was both s...\n",
              "296    The awful experience I had with the staff has ...\n",
              "297    The delay in treatment had a harmful effect on...\n",
              "298    I hate to say it, but the service here has det...\n",
              "299    Witnessing cops within the hospital premises w...\n",
              "Name: Comment, Length: 300, dtype: object"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# prompt: read the patient_review_data.csv and store  in a df\n",
        "# Read the CSV file into a DataFrame.\n",
        "# Private data source\n",
        "df = pd.read_csv('../Data/patient_review_data.csv')\n",
        "df['Comment']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YKWIq1vvIICs",
        "outputId": "aa83398b-887d-4d80-b7a3-cbb12425f3c4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0      15 Highly Important Questions About Adulthood,...\n",
              "1      250 Nuns Just Cycled All The Way From Kathmand...\n",
              "2      Australian comedians \"could have been shot\" du...\n",
              "3      Lycos launches screensaver to increase spammer...\n",
              "4      Fußball-Bundesliga 2008–09: Goalkeeper Butt si...\n",
              "                             ...                        \n",
              "495    '90s Cartoons With Human Eyes May Ruin Your Ch...\n",
              "496    Wait, WTF Is \"I Saw Mommy Kissing Santa Claus\"...\n",
              "497           First Women Win Seats in Kuwait Parliament\n",
              "498                   Were You A Scene Kid Or An Emo Kid\n",
              "499    Opposition leader takes early lead in Sierra L...\n",
              "Name: Comment, Length: 500, dtype: object"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = pd.read_csv('book_title_test_data.csv')\n",
        "# Keep only the first 500 rows in df\n",
        "df = df.iloc[:500]\n",
        "\n",
        "# Rename 'title' column to 'Comment'\n",
        "df.rename(columns={'title': 'Comment'}, inplace=True)\n",
        "df['Comment']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v94BDR8L6xmP",
        "outputId": "03ea42c9-7569-43c7-8cf8-a39174950db3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     /Users/sajjadislam/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "from transformers import BertTokenizer, BertModel\n",
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from gensim.models.coherencemodel import CoherenceModel\n",
        "from gensim.corpora.dictionary import Dictionary\n",
        "\n",
        "import nltk\n",
        "from collections import Counter\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "import string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "meoSwXL76pzh"
      },
      "outputs": [],
      "source": [
        "# Load pre-trained BERT model and tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Function to encode comments into embeddings\n",
        "def encode_comments(texts):\n",
        "    encoded_input = tokenizer(texts, padding=True, truncation=True, return_tensors='pt')\n",
        "    with torch.no_grad():\n",
        "        model_output = model(**encoded_input)\n",
        "    embeddings = model_output.last_hidden_state.mean(dim=1)\n",
        "    return embeddings\n",
        "\n",
        "# Encode comments\n",
        "embeddings = encode_comments(df['Comment'].tolist())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "0_WpHexiIDSo"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8wDLu8szbENx",
        "outputId": "73de57eb-cdb7-4f87-ab6c-d8a6cdff4bf1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /Users/sajjadislam/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     /Users/sajjadislam/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     /Users/sajjadislam/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# Download required NLTK resources\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Initialize BERT tokenizer and model\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Set of English stopwords and initialize lemmatizer\n",
        "stop_words = set(stopwords.words('english'))\n",
        "custom_stop_words = ['the', 'and', 'was', 'were', 'with', 'a', 'my', 'to']\n",
        "stop_words.update(custom_stop_words)\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def encode_comments(texts):\n",
        "    # Function to remove stopwords\n",
        "    def remove_stopwords(text):\n",
        "        word_tokens = word_tokenize(text)\n",
        "        filtered_text = [word for word in word_tokens if word.lower() not in stop_words]\n",
        "        return \" \".join(filtered_text)\n",
        "\n",
        "    # Function to remove stopwords and lemmatize\n",
        "    import string\n",
        "\n",
        "    def preprocess_text(text):\n",
        "        # Remove punctuation from text\n",
        "        translator = str.maketrans('', '', string.punctuation)\n",
        "        text = text.translate(translator).lower()  # Lowercasing the text here\n",
        "\n",
        "        word_tokens = word_tokenize(text)\n",
        "        filtered_and_lemmatized_text = [\n",
        "            lemmatizer.lemmatize(word)\n",
        "            for word in word_tokens\n",
        "            if word not in stop_words\n",
        "        ]\n",
        "        return \" \".join(filtered_and_lemmatized_text)\n",
        "\n",
        "    # Remove stopwords from each comment\n",
        "    cleaned_texts = [preprocess_text(text) for text in texts]\n",
        "\n",
        "    # Tokenization and encoding\n",
        "    encoded_input = tokenizer(cleaned_texts, padding=True, truncation=True, return_tensors='pt')\n",
        "    with torch.no_grad():\n",
        "        model_output = model(**encoded_input)\n",
        "    embeddings = model_output.last_hidden_state.mean(dim=1)\n",
        "    return embeddings\n",
        "\n",
        "# Encode comments\n",
        "embeddings = encode_comments(df['Comment'].tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rBRy7kOhSt-M",
        "outputId": "58c9f7d6-c982-4286-b431-ad4dda7fe1de"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/sajjadislam/opt/anaconda3/envs/py312_topic_modeling/lib/python3.12/site-packages/threadpoolctl.py:1214: RuntimeWarning: \n",
            "Found Intel OpenMP ('libiomp') and LLVM OpenMP ('libomp') loaded at\n",
            "the same time. Both libraries are known to be incompatible and this\n",
            "can cause random crashes or deadlocks on Linux when loaded in the\n",
            "same Python program.\n",
            "Using threadpoolctl may cause crashes or deadlocks. For more\n",
            "information and possible workarounds, please see\n",
            "    https://github.com/joblib/threadpoolctl/blob/master/multiple_openmp.md\n",
            "\n",
            "  warnings.warn(msg, RuntimeWarning)\n"
          ]
        }
      ],
      "source": [
        "# Perform clustering on embeddings (example: K-Means)\n",
        "num_topics = 3  # Set the number of topics\n",
        "kmeans = KMeans(n_clusters=num_topics, random_state=0).fit(embeddings.numpy())\n",
        "df['topic'] = kmeans.labels_\n",
        "\n",
        "# For Coherence Score, transform BERT embeddings to bag-of-words\n",
        "texts = [comment.split() for comment in df['Comment'].tolist()]\n",
        "dictionary = Dictionary(texts)\n",
        "corpus = [dictionary.doc2bow(text) for text in texts]\n",
        "\n",
        "# Use KMeans centroids to find closest comments for each topic\n",
        "def get_representative_comments(topic_idx, n_representative=10):\n",
        "    indices = np.where(df['topic'] == topic_idx)[0]\n",
        "    centroid = kmeans.cluster_centers_[topic_idx]\n",
        "    distances = cosine_similarity([centroid], embeddings[indices].numpy())\n",
        "    closest_indices = np.argsort(distances[0])[:n_representative]\n",
        "    return [df.iloc[indices[i]]['Comment'] for i in closest_indices]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J3ZlFFoS-lYU",
        "outputId": "48334e8f-b921-4b08-cd7e-493d42498feb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[['insulted', 'staff', 'hipaa'],\n",
              " ['staff', 'unresolved', 'behavior'],\n",
              " ['time', 'care', 'doctors']]"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def get_most_frequent_words(comments, top_n=3):\n",
        "    # Tokenize words, convert to lower case, and filter out stop words and punctuation\n",
        "    words = [word for comment in comments\n",
        "             for word in word_tokenize(comment.lower())\n",
        "             if word not in stop_words and word not in string.punctuation]\n",
        "\n",
        "    # Find the most common words\n",
        "    most_common = [word for word, freq in Counter(words).most_common(top_n)]\n",
        "    return most_common\n",
        "\n",
        "# Create lists of most frequent words representing each topic\n",
        "topic_words = [get_most_frequent_words(get_representative_comments(i)) for i in range(num_topics)]\n",
        "topic_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M3jlDkyjAHN1",
        "outputId": "9ef149b5-b920-46b7-b0d1-da27c937acbf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Coherence Score: 0.6358999695610196\n"
          ]
        }
      ],
      "source": [
        "from gensim.models.coherencemodel import CoherenceModel\n",
        "\n",
        "cm = CoherenceModel(topics=topic_words, texts=texts, dictionary=dictionary, coherence='c_v')\n",
        "#cm = CoherenceModel(topics=topic_words, dictionary=dictionary, coherence='u_mass')\n",
        "coherence_score = cm.get_coherence()\n",
        "print(f'Coherence Score: {coherence_score}')\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "py312_topic_modeling",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
