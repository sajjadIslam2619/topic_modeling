{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Datasource: https://github.com/anirudhshenoy/text-classification-small-datasets/tree/master/datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "s1-zZAHWIj3x"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /Users/sajjadislam/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     /Users/sajjadislam/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     /Users/sajjadislam/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from gensim.models.coherencemodel import CoherenceModel\n",
        "from gensim.corpora.dictionary import Dictionary\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "import string\n",
        "import torch\n",
        "import nltk\n",
        "\n",
        "# Download required NLTK resources\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(450, 1)"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load your data\n",
        "df = pd.read_csv('../Data/Final_Synthetic_Patient_Feedback_Dataset.csv')\n",
        "\n",
        "# Drop rows with missing titles and preprocess\n",
        "df.dropna(subset=['Patient Feedback'], inplace=True)\n",
        "df.rename(columns={'Patient Feedback': 'Comment'}, inplace=True)\n",
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set up stop words and lemmatizer\n",
        "stop_words = set(stopwords.words('english'))\n",
        "custom_stop_words = set(string.digits + string.punctuation)\n",
        "additional_stop_words = ['the', 'and', 'was', 'were', 'with', 'a', 'my', '``']\n",
        "stop_words.update(custom_stop_words)\n",
        "stop_words.update(additional_stop_words)\n",
        "lemmatizer = WordNetLemmatizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to preprocess text\n",
        "def preprocess_text(text):\n",
        "    translator = str.maketrans('', '', string.punctuation)\n",
        "    text = text.translate(translator).lower()  # Remove punctuation and lowercase\n",
        "    word_tokens = word_tokenize(text)\n",
        "    return \" \".join(\n",
        "        lemmatizer.lemmatize(word)\n",
        "        for word in word_tokens\n",
        "        if word not in stop_words\n",
        "    )\n",
        "\n",
        "# Preprocess and encode comments\n",
        "df['Comment'] = df['Comment'].apply(preprocess_text)\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "def encode_comments(texts):\n",
        "    encoded_input = tokenizer(texts, padding=True, truncation=True, return_tensors='pt')\n",
        "    with torch.no_grad():\n",
        "        model_output = model(**encoded_input)\n",
        "    embeddings = model_output.last_hidden_state.mean(dim=1)\n",
        "    return embeddings\n",
        "\n",
        "embeddings = encode_comments(df['Comment'].tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the range of num_topics and top_n values\n",
        "num_topics_list = [3, 5, 10, 15, 20]\n",
        "top_n_list = [5, 10, 15, 20]\n",
        "\n",
        "# Stop words and punctuation removal\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "# Store coherence scores\n",
        "coherence_scores = []\n",
        "\n",
        "for num_topics in num_topics_list:\n",
        "    kmeans = KMeans(n_clusters=num_topics, random_state=0, n_init=10).fit(embeddings)\n",
        "    df[\"topic\"] = kmeans.labels_\n",
        "\n",
        "    # Prepare texts and dictionary for coherence calculation\n",
        "    texts = [comment.split() for comment in df[\"Comment\"].tolist()]\n",
        "    dictionary = Dictionary(texts)\n",
        "    corpus = [dictionary.doc2bow(text) for text in texts]\n",
        "\n",
        "    def get_most_frequent_words_for_topic(topic_idx, top_n=5):\n",
        "        \"\"\"Get the most frequent words for a given topic.\"\"\"\n",
        "        topic_comments = df[df[\"topic\"] == topic_idx][\"Comment\"].tolist()\n",
        "        words = [\n",
        "            word\n",
        "            for comment in topic_comments\n",
        "            for word in word_tokenize(comment.lower())\n",
        "            if word not in stop_words and word not in string.punctuation\n",
        "        ]\n",
        "        return [word for word, freq in Counter(words).most_common(top_n)]\n",
        "\n",
        "    for top_n in top_n_list:\n",
        "        topic_words = [get_most_frequent_words_for_topic(i, top_n) for i in range(num_topics)]\n",
        "\n",
        "        # Calculate coherence score\n",
        "        cm = CoherenceModel(topics=topic_words, texts=texts, dictionary=dictionary, coherence=\"c_v\")\n",
        "        coherence_score = cm.get_coherence()\n",
        "\n",
        "        # Store results\n",
        "        coherence_scores.append({\n",
        "            \"num_topics\": num_topics,\n",
        "            \"top_n\": top_n,\n",
        "            \"coherence\": coherence_score\n",
        "        })\n",
        "\n",
        "# Convert results into a dataframe for better visualization\n",
        "df_coherence = pd.DataFrame(coherence_scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    num_topics  top_n  coherence\n",
            "0            3      5   0.905850\n",
            "1            3     10   0.704385\n",
            "2            3     15   0.375007\n",
            "3            3     20   0.313978\n",
            "4            5      5   0.947034\n",
            "5            5     10   0.585847\n",
            "6            5     15   0.419717\n",
            "7            5     20   0.325357\n",
            "8           10      5   0.904887\n",
            "9           10     10   0.614634\n",
            "10          10     15   0.465492\n",
            "11          10     20   0.400925\n",
            "12          15      5   0.848865\n",
            "13          15     10   0.573744\n",
            "14          15     15   0.445961\n",
            "15          15     20   0.381808\n",
            "16          20      5   0.892667\n",
            "17          20     10   0.608621\n",
            "18          20     15   0.471359\n",
            "19          20     20   0.420186\n"
          ]
        }
      ],
      "source": [
        "# Display the dataframe\n",
        "print(df_coherence)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "py312_topic_modeling",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
