{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Datasource: https://github.com/anirudhshenoy/text-classification-small-datasets/tree/master/datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "s1-zZAHWIj3x"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/sajjadislam/opt/anaconda3/envs/py312_topic_modeling/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /Users/sajjadislam/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     /Users/sajjadislam/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     /Users/sajjadislam/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from gensim.models.coherencemodel import CoherenceModel\n",
        "from gensim.corpora.dictionary import Dictionary\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "import string\n",
        "import torch\n",
        "import nltk\n",
        "\n",
        "# Download required NLTK resources\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(500, 2)"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load your data\n",
        "#df = pd.read_csv('../Data/YT_title_test_data.csv')\n",
        "df = df = pd.read_csv('../Data/YT_title_test_data_500.csv')\n",
        "\n",
        "# Drop rows with missing titles and preprocess\n",
        "df.dropna(subset=['title'], inplace=True)\n",
        "df.rename(columns={'title': 'Comment'}, inplace=True)\n",
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set up stop words and lemmatizer\n",
        "stop_words = set(stopwords.words('english'))\n",
        "custom_stop_words = set(string.digits + string.punctuation)\n",
        "additional_stop_words = ['the', 'and', 'was', 'were', 'with', 'a', 'my', '``']\n",
        "stop_words.update(custom_stop_words)\n",
        "stop_words.update(additional_stop_words)\n",
        "lemmatizer = WordNetLemmatizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to preprocess text\n",
        "def preprocess_text(text):\n",
        "    translator = str.maketrans('', '', string.punctuation)\n",
        "    text = text.translate(translator).lower()  # Remove punctuation and lowercase\n",
        "    word_tokens = word_tokenize(text)\n",
        "    return \" \".join(\n",
        "        lemmatizer.lemmatize(word)\n",
        "        for word in word_tokens\n",
        "        if word not in stop_words\n",
        "    )\n",
        "\n",
        "# Preprocess and encode comments\n",
        "df['Comment'] = df['Comment'].apply(preprocess_text)\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "def encode_comments(texts):\n",
        "    encoded_input = tokenizer(texts, padding=True, truncation=True, return_tensors='pt')\n",
        "    with torch.no_grad():\n",
        "        model_output = model(**encoded_input)\n",
        "    embeddings = model_output.last_hidden_state.mean(dim=1)\n",
        "    return embeddings\n",
        "\n",
        "embeddings = encode_comments(df['Comment'].tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Coherence Score: 0.34728591483563503\n"
          ]
        }
      ],
      "source": [
        "# Perform KMeans clustering\n",
        "num_topics = 20\n",
        "kmeans = KMeans(n_clusters=num_topics, random_state=0).fit(embeddings.numpy())\n",
        "df['topic'] = kmeans.labels_\n",
        "\n",
        "# Prepare texts and dictionary for coherence calculation\n",
        "texts = [comment.split() for comment in df['Comment'].tolist()]\n",
        "dictionary = Dictionary(texts)\n",
        "corpus = [dictionary.doc2bow(text) for text in texts]\n",
        "\n",
        "# Find representative comments and topic words\n",
        "def get_representative_comments(topic_idx, n_representative=10):\n",
        "    indices = np.where(df['topic'] == topic_idx)[0]\n",
        "    centroid = kmeans.cluster_centers_[topic_idx]\n",
        "    distances = cosine_similarity([centroid], embeddings[indices].numpy())\n",
        "    closest_indices = np.argsort(distances[0])[:n_representative]\n",
        "    return [df.iloc[indices[i]]['Comment'] for i in closest_indices]\n",
        "\n",
        "def get_most_frequent_words(comments, top_n=5):\n",
        "    words = [\n",
        "        word\n",
        "        for comment in comments\n",
        "        for word in word_tokenize(comment.lower())\n",
        "        if word not in stop_words and word not in string.punctuation\n",
        "    ]\n",
        "    return [word for word, freq in Counter(words).most_common(top_n)]\n",
        "\n",
        "def get_most_frequent_words_for_topic(topic_idx, top_n=5):\n",
        "    topic_comments = df[df['topic'] == topic_idx]['Comment'].tolist()  # Get all comments for the topic\n",
        "    words = [\n",
        "        word\n",
        "        for comment in topic_comments\n",
        "        for word in word_tokenize(comment.lower())\n",
        "        if word not in stop_words and word not in string.punctuation\n",
        "    ]\n",
        "    return [word for word, freq in Counter(words).most_common(top_n)]\n",
        "\n",
        "topic_words = [get_most_frequent_words_for_topic(i) for i in range(num_topics)]\n",
        "\n",
        "#topic_words = [get_most_frequent_words(get_representative_comments(i)) for i in range(num_topics)]\n",
        "\n",
        "#coherance score issue: https://github.com/piskvorky/gensim/issues/3328\n",
        "# Calculate coherence score\n",
        "cm = CoherenceModel(topics=topic_words, texts=texts, dictionary=dictionary, coherence='c_v')\n",
        "coherence_score = cm.get_coherence()\n",
        "print(f'Coherence Score: {coherence_score}')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "py312_topic_modeling",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
